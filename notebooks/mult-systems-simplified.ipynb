{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89cbb521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "238290de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/achung/Desktop/cs338/dr-know-it-all/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 0.99k/0.99k [00:00<00:00, 477kB/s]\n",
      "Downloading: 100%|██████████| 487M/487M [00:09<00:00, 52.4MB/s] \n",
      "Downloading: 100%|██████████| 167/167 [00:00<00:00, 114kB/s]\n",
      "Downloading: 100%|██████████| 873/873 [00:00<00:00, 701kB/s]\n",
      "Downloading: 100%|██████████| 779k/779k [00:00<00:00, 4.34MB/s]\n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 3.25MB/s]\n",
      "Downloading: 100%|██████████| 90.0/90.0 [00:00<00:00, 58.5kB/s]\n",
      "Downloading: 100%|██████████| 487M/487M [00:09<00:00, 51.4MB/s] \n",
      "Downloading: 100%|██████████| 236/236 [00:00<00:00, 153kB/s]\n",
      "Downloading: 100%|██████████| 779k/779k [00:00<00:00, 4.17MB/s]\n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 3.17MB/s]\n",
      "Downloading: 100%|██████████| 2.01M/2.01M [00:00<00:00, 9.48MB/s]\n",
      "Downloading: 100%|██████████| 90.0/90.0 [00:00<00:00, 74.2kB/s]\n",
      "Downloading: 100%|██████████| 0.99k/0.99k [00:00<00:00, 672kB/s]\n",
      "Downloading: 100%|██████████| 487M/487M [00:35<00:00, 14.4MB/s] \n",
      "Downloading: 100%|██████████| 167/167 [00:00<00:00, 50.5kB/s]\n",
      "Downloading: 100%|██████████| 873/873 [00:00<00:00, 233kB/s]\n",
      "Downloading: 100%|██████████| 779k/779k [00:00<00:00, 3.54MB/s]\n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 1.47MB/s]\n",
      "Downloading: 100%|██████████| 90.0/90.0 [00:00<00:00, 15.5kB/s]\n",
      "Downloading: 100%|██████████| 487M/487M [00:22<00:00, 22.7MB/s] \n",
      "Downloading: 100%|██████████| 167/167 [00:00<00:00, 63.9kB/s]\n",
      "Downloading: 100%|██████████| 873/873 [00:00<00:00, 312kB/s]\n",
      "Downloading: 100%|██████████| 779k/779k [00:00<00:00, 2.98MB/s]\n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 2.43MB/s]\n",
      "Downloading: 100%|██████████| 90.0/90.0 [00:00<00:00, 23.5kB/s]\n",
      "Downloading: 100%|██████████| 487M/487M [00:37<00:00, 13.8MB/s] \n"
     ]
    }
   ],
   "source": [
    "joetokenizer = AutoTokenizer.from_pretrained(\"huggingtweets/joebiden\")\n",
    "joemodel = AutoModelForCausalLM.from_pretrained(\"huggingtweets/joebiden\")\n",
    "donaldtokenizer = AutoTokenizer.from_pretrained(\"huggingtweets/realdonaldtrump\")\n",
    "donaldmodel = AutoModelForCausalLM.from_pretrained(\"huggingtweets/realdonaldtrump\")\n",
    "elontokenizer = AutoTokenizer.from_pretrained(\"huggingtweets/elonmusk\")\n",
    "elonmodel = AutoModelForCausalLM.from_pretrained(\"huggingtweets/elonmusk\")\n",
    "tstokenizer = AutoTokenizer.from_pretrained(\"huggingtweets/tswiftlyricsbot\")\n",
    "tsmodel = AutoModelForCausalLM.from_pretrained(\"huggingtweets/tswiftlyricsbot\")\n",
    "buddhatokenizer = AutoTokenizer.from_pretrained(\"huggingtweets/_buddha_quotes\")\n",
    "buddhamodel = AutoModelForCausalLM.from_pretrained(\"huggingtweets/_buddha_quotes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "20b61b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "people = {\n",
    "    \"joe\": (joemodel, joetokenizer),\n",
    "    \"donald\": (donaldmodel, donaldtokenizer),\n",
    "    \"elon\": (elonmodel, elontokenizer),\n",
    "    \"ts\": (tsmodel, tstokenizer),\n",
    "    \"buddha\": (buddhamodel, buddhatokenizer)\n",
    "}\n",
    "prompt = \"Can you tell a funny story?\"\n",
    "\n",
    "while True:\n",
    "    is_end = False\n",
    "    for key, (model, tokenizer) in people.items():\n",
    "        generator = pipeline('text-generation', model=model,\n",
    "                             tokenizer=tokenizer, max_length=max(15, len(prompt)))\n",
    "        text = generator(prompt, num_return_sequences=5)\n",
    "        rand_num = random.randint(0, 4)\n",
    "        new_text = text[rand_num]['generated_text'].split(prompt)[1]\n",
    "        if len(new_text) <= 1: # if new text is the last token (or no token), try again\n",
    "            rand_num = random.randint(0, 4)\n",
    "            new_text = text[rand_num]['generated_text'].split(prompt)[1]\n",
    "        if len(new_text) <= 1:  # if still the last token (punctuation or stop), end\n",
    "            prompt += new_text\n",
    "            is_end = True\n",
    "            break\n",
    "        else:  # add first word from new text\n",
    "            prompt += ' ' + new_text.split(' ')[1]\n",
    "\n",
    "    if is_end:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "957f9c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you tell a funny story? @realDonaldTrump. It hard to do. But you have to do it. Find out. Tell it to @PeteTatemaz. If he tells you what he knows, he will. @BreitbartNews will be the only one who knows.'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
